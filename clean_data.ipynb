{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning notebook\n",
    "\n",
    "- do cleaning\n",
    "- basically spellcheck got rid of ~1000 unique words which is minor improvement but removing slang, stopwords, punctuation, urls should help even more\n",
    "- jamspell was the best spellchecker between textblob, symspell, and jamspell but its also not perfect sadly\n",
    "\n",
    "Notes about below code:\n",
    "- reformatted the slang in txt called slang_edit.txt\n",
    "- replaced all slang with definition\n",
    "- removed punctuation using string.punctuation, then removed additional randowm punctuation like \"''\", \"--\", \"``\", \"...\"\n",
    "- replaced website URLs with \"website\"\n",
    "- removed stopwords\n",
    "- in total removed ~600 unique words\n",
    "\n",
    "Decisions to be made/things to fix:\n",
    "- are we happy with how contractions are being treated?\n",
    "- do we want to remove \"I\" and/or \"you\" from the stopwords list?\n",
    "\n",
    "\n",
    "Iris \n",
    "- added contractions to slang list -- ex. 'can't' : 'can not' \n",
    "\n",
    "Lydia\n",
    "- removed {'i', 'you', 'they', 'them', 'me', 'what', 'want', 'go', 'of', 'to', 'let', 'get', 'got', 'not'} from stop word list? My vote is to keep words like \"i\" and \"you\" + words that do add context/sentiment like \"not\" \n",
    "\n",
    "Anna\n",
    "- added \"propane\" (actually propname after autocorrect) to stopwords? how do we feel about that... \n",
    "\n",
    "Diane\n",
    "- rearranged order of cleaning slightly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in any necessary libs\n",
    "import pandas as pd \n",
    "# nltk langs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the spellchecked csv file: 'en_personality_spellcheck.csv'\n",
    "df = pd.read_csv('en_personality_spellcheck.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slang_dict = {}\n",
    "#read in slang txt\n",
    "with open(\"slang_edit.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            slang_word, meaning = line.split(':')\n",
    "            slang_dict[slang_word.strip().strip(\"'\")] = meaning.strip().strip(\"'\")\n",
    "\n",
    "        \n",
    "def replace_slang(tokens):\n",
    "    return [slang_dict[token] if token in slang_dict.keys() else token for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove punctuation. replace all ... with >3 dots with ... and replace all URLs with \"website\"\n",
    "punc = string.punctuation #removed < so that hearts wouldn't be removed\n",
    "punc = punc.replace(\"<\",\"\")\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    # Remove any \"//\" from tokens and split the token into two tokens\n",
    "    tokens = ['website' if (token.count('www.') == 1 or token.count('.com')==1 or token.count('.org')==1) else token for token in tokens]\n",
    "    tokens = [subtoken for token in tokens for subtoken in token.split(\"//\")]\n",
    "    tokens = [token.replace(\"-\", \"\") for token in tokens]\n",
    "    tokens = [token.replace(\" \", \"\") for token in tokens]\n",
    "    tokens = [token.replace(\".\", \"\") for token in tokens] #might want to leave in periods at ends of sentences...\n",
    "    tokens = ['...' if token.count('.') > 3 else token for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in [\"''\", \"--\", \"``\", \"n't\",\"...\"]]\n",
    "    tokens = [re.sub(r'(.)\\1{3,}', r'\\1\\1', token) for token in tokens]\n",
    "    # tokens = ['website' if (token.count('www.') == 1 or token.count('.com')==1 or token.count('.org')==1) else token for token in tokens]\n",
    "    return [token for token in tokens if token not in punc]\n",
    "#remove preset stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    add_stop_words = {\"n't\", \"'s\", \"'m\"}\n",
    "    stop_words.update(add_stop_words)\n",
    "    words_to_remove = {'i', 'you', 'they', 'them', 'me', 'what', 'want', 'go', 'of', 'to', 'let', 'get', 'got', 'not'}\n",
    "    for word in words_to_remove:\n",
    "        if word in stop_words:\n",
    "            stop_words.remove(word)\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text) #' '.join(text)\n",
    "    tokens = replace_slang(tokens) #replace slang with defs\n",
    "    tokens = remove_punctuation(tokens) #remove punctuation\n",
    "    combined_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens): #recombine hearts\n",
    "        if tokens[i] == '<' and i+1 < len(tokens) and tokens[i+1] == '3':\n",
    "            combined_tokens.append('love')\n",
    "            i += 2\n",
    "        else:\n",
    "            combined_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return ' '.join(combined_tokens)\n",
    "\n",
    "df['corrected_StopsPuncSlang'] = df['corrected'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              likes the sound of thunder\n",
       "1       is so sleepy it 's not even funny that 's she ...\n",
       "2       is more and wants the knot of muscles at the b...\n",
       "3               likes how the day sounds in this new song\n",
       "4                                            is home love\n",
       "                              ...                        \n",
       "9115                                    ahh it 's snowing\n",
       "9116                                           its friday\n",
       "9117                                             go likes\n",
       "9118                                      best night ever\n",
       "9119                                                  red\n",
       "Name: corrected_StopsPuncSlang, Length: 9120, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corrected_StopsPuncSlang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original # unique words 15315\n",
      "Spellchecked # unique words: 14461\n",
      "Stopwords, slang, and punct removed unique words: 13903\n",
      "Original total words: 174366\n",
      "Post-processing total words: 142121\n"
     ]
    }
   ],
   "source": [
    "before = df['STATUS'].apply(word_tokenize).explode().unique()\n",
    "print(\"Original # unique words\", len(before))\n",
    "after = df['corrected'].apply(word_tokenize).explode().unique()\n",
    "print(\"Spellchecked # unique words:\", len(after))\n",
    "new = df['corrected_StopsPuncSlang'].apply(word_tokenize).explode().unique()\n",
    "print(\"Stopwords, slang, and punct removed unique words:\", len(new))\n",
    "print(\"Original total words:\", len(df['STATUS'].apply(word_tokenize).explode()))\n",
    "print(\"Post-processing total words:\", len(df['corrected_StopsPuncSlang'].apply(word_tokenize).explode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            likes the sound of thunder\n",
       "1     is so sleepy it 's not even funny that 's she ...\n",
       "2     is more and wants the knot of muscles at the b...\n",
       "3             likes how the day sounds in this new song\n",
       "4                                          is home love\n",
       "5                                               website\n",
       "6     saw a nun zombie and liked it also name tentac...\n",
       "7     is in kentucky 421 miles into her 1100 mile jo...\n",
       "8     was about to finish a digital painting before ...\n",
       "9     is celebrating her new haircut by listening to...\n",
       "10                     has a crush on the green lantern\n",
       "11                               has magic on the brain\n",
       "12    saw transformers up and year one this week goo...\n",
       "13    who wants to meet up on schedule pickup day at...\n",
       "14         desires the thrill of inspiration also money\n",
       "15                          is going to be at 9:30 yeah\n",
       "16    is reading admiring her permit and occasionall...\n",
       "17    thinks intangibility should be an option in re...\n",
       "18              is tired name let me go to sleep please\n",
       "19          is discovering the many flavors of insomnia\n",
       "20    is watching cousin play computer game on telev...\n",
       "21    why is it i 'm only getting the urge to draw w...\n",
       "22    who'da thought a single text message could be ...\n",
       "23    wishes to develop a super power that prevents ...\n",
       "24                            tell me what to draw plot\n",
       "25    found a bunny bunny died buried bunny now is d...\n",
       "26                          is just about insane by now\n",
       "27    wants to sleep eight hours tonight and so she ...\n",
       "28    really hates hormones and emotions right now w...\n",
       "29    has a watch that matches her glasses she wishe...\n",
       "Name: corrected_StopsPuncSlang, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corrected_StopsPuncSlang'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleanedCOPY.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code checks the number of unique users, and that the personalities given to each user is the same among all of their posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of unique users\n",
    "print(f\"There are {len(df['#AUTHID'].unique())} unique users.\")\n",
    "\n",
    "columns_to_check = [\"sEXT\", \"sNEU\", \"sAGR\", \"sCON\", \"sOPN\", \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n",
    "\n",
    "# Grouping by '#AUTHID' and checking if all rows within each group have the same values in the specified columns\n",
    "result = df.groupby('#AUTHID').apply(lambda x: (x[columns_to_check] == x[columns_to_check].iloc[0]).all().all())\n",
    "\n",
    "# Checking if any of the results are False\n",
    "any_false = not result.all()\n",
    "\n",
    "# Displaying the result\n",
    "print(f\"Are there any columns that have different typings: {any_false}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will merge all of the text for each user on the column that has cleaned punctuation. The other columns will also be transferred as there is no different data among users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store the result\n",
    "grouped = pd.DataFrame(columns=['#AUTHID', 'text', 'sNEU', 'cNEU'])\n",
    "\n",
    "# Iterate over unique #AUTHIDs\n",
    "for authid in df['#AUTHID'].unique():\n",
    "    # Filter the DataFrame for the current #AUTHID\n",
    "    temp_df = df[df['#AUTHID'] == authid]\n",
    "\n",
    "    # Fill NaN values in 'corrected_StopsPuncSlang' with empty strings\n",
    "    temp_df['corrected_StopsPuncSlang'] = temp_df['corrected_StopsPuncSlang'].fillna('')\n",
    "\n",
    "    # Concatenate the 'corrected_StopsPuncSlang' text\n",
    "    text = ' '.join(temp_df['corrected_StopsPuncSlang'])\n",
    "    # Choose Neuropathy to copy\n",
    "    sNEU_value = temp_df['sNEU'].iloc[0]  # Assuming it's the same for all rows of the same #AUTHID\n",
    "\n",
    "    cNEU_value = temp_df['cNEU'].iloc[0]\n",
    "\n",
    "    # Create a DataFrame with the current #AUTHID, concatenated text, and copied 'sEXT' value\n",
    "    result_df = pd.DataFrame({'#AUTHID': [authid], 'text': [text], 'sNEU': [sNEU_value], 'cNEU': [cNEU_value]})\n",
    "    # Concatenate the result DataFrame with the new_df\n",
    "    grouped = pd.concat([grouped, result_df], ignore_index=True)\n",
    "\n",
    "# Displaying the new DataFrame\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up grouped text into rows of max 100 words each\n",
    "def get_split(text1):\n",
    "  l_total = []\n",
    "  l_parcial = []\n",
    "  if len(text1.split())//50 >0:\n",
    "    n = len(text1.split())//50\n",
    "  else: \n",
    "    n = 1\n",
    "  for w in range(n):\n",
    "    if w == 0:\n",
    "      l_parcial = text1.split()[:100]\n",
    "      l_total.append(\" \".join(l_parcial))\n",
    "    else:\n",
    "      l_parcial = text1.split()[w*50:w*50 + 100]\n",
    "      l_total.append(\" \".join(l_parcial))\n",
    "  return l_total\n",
    "\n",
    "grouped['textsplt'] = grouped['text'].apply(get_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ungroup grouped text lol\n",
    "group = grouped.explode('textsplt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.to_csv(\"group.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
